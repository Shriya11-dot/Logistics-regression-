{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 -  What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "\n",
        "Answer - Logistic Regression is a statistical method used to predict categorical outcomes, usually binary (like yes/no, spam/not spam). Instead of predicting a number like Linear Regression, it predicts the probability of a certain class or event.\n",
        "\n",
        "Linear Regression gives a straight-line output and is used when the target is a continuous number (like price or height). Logistic Regression, on the other hand, uses the logistic (sigmoid) function to squeeze the output between 0 and 1, making it perfect for classification tasks."
      ],
      "metadata": {
        "id": "4l3eifcGE6dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 - Explain the role of the Sigmoid function in Logistic Regression\n",
        "\n",
        "Answer - The **sigmoid function** plays a key role in **Logistic Regression** because it transforms the linear output into a **probability** between **0 and 1**.\n",
        "\n",
        "How it works:\n",
        "\n",
        "* Logistic Regression first calculates a **linear combination** of input features (like Linear Regression):\n",
        "  `z = w₁x₁ + w₂x₂ + ... + b`\n",
        "\n",
        "* Then, it passes `z` through the **sigmoid function**:\n",
        "  `sigmoid(z) = 1 / (1 + e^(-z))`\n",
        "\n",
        "This squashes the output to a range between **0 and 1**, which can be interpreted as the **probability** that the input belongs to the **positive class** (e.g., \"yes\" or \"1\").\n",
        "\n",
        "So, the sigmoid function makes it possible for Logistic Regression to **predict probabilities** instead of raw numbers.\n"
      ],
      "metadata": {
        "id": "Br9r-OOCFdWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 - What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer - Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty to the model for having large or complex coefficients.\n",
        "\n",
        "**Why it's needed**:\n",
        "\n",
        "Without regularization, the model might fit the training data too well, including noise, which hurts its performance on new, unseen data.\n",
        "\n",
        "Regularization helps the model stay simpler and more general, improving its ability to make accurate predictions on test data.\n",
        "\n",
        "**How it works**:\n",
        "\n",
        "It adds a penalty term to the cost function:\n",
        "\n",
        "L1 regularization (Lasso) adds the absolute values of coefficients.\n",
        "\n",
        "L2 regularization (Ridge) adds the squares of the coefficients.\n",
        "\n",
        "This discourages the model from assigning too much weight to any one feature."
      ],
      "metadata": {
        "id": "AZc1usgpF_cF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 - What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "\n",
        "Answer - Here are some common evaluation metrics for classification models and why they matter:\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "**What it measures**: % of correct predictions out of total predictions.\n",
        "\n",
        "**Why it's important**: Gives a quick snapshot of model performance — but can be misleading with imbalanced datasets.\n",
        "\n",
        "2. Precision\n",
        "\n",
        "**What it measures**: Of all predicted positives, how many were actually positive?\n",
        "\n",
        "**Why it's important**: Useful when false positives are costly (e.g., spam filters).\n",
        "\n",
        "3. Recall (Sensitivity)\n",
        "\n",
        "**What it measures**: Of all actual positives, how many did the model correctly identify?\n",
        "\n",
        "**Why it's important**: Crucial when missing a positive case is risky (e.g., detecting diseases).\n",
        "\n",
        "4. F1 Score\n",
        "\n",
        "**What it measures**: Harmonic mean of precision and recall.\n",
        "\n",
        "**Why it's important**: A good balance when you need to consider both precision and recall.\n",
        "\n",
        "5. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
        "\n",
        "**What it measures**: How well the model separates classes across all thresholds.\n",
        "\n",
        "**Why it's important**: Helps compare models regardless of the chosen probability cutoff"
      ],
      "metadata": {
        "id": "dlps71SEGbR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 - Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy."
      ],
      "metadata": {
        "id": "5YqL0Pd-Ide2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Program: Logistic Regression with sklearn dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"First 5 rows of dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)  # increase iterations for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nModel Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVcZmUnHJVB7",
        "outputId": "fe3ddd36-e495-477c-f9e2-4d1df30737cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of dataset:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6 - Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy."
      ],
      "metadata": {
        "id": "auaUeaMIJrF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Program: Logistic Regression with L2 regularization (Ridge)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print coefficients and accuracy\n",
        "print(\"Model Coefficients (first 10 shown):\")\n",
        "print(model.coef_[0][:10])  # show first 10 coefficients for readability\n",
        "\n",
        "print(\"\\nModel Intercept:\")\n",
        "print(model.intercept_)\n",
        "\n",
        "print(\"\\nModel Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSy2bnv2Jw9_",
        "outputId": "5a134369-2da9-4272-cf64-c298ad3d4da1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (first 10 shown):\n",
            "[ 1.0274368   0.22145051 -0.36213488  0.0254667  -0.15623532 -0.23771256\n",
            " -0.53255786 -0.28369224 -0.22668189 -0.03649446]\n",
            "\n",
            "Model Intercept:\n",
            "[28.64871395]\n",
            "\n",
            "Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7 - Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report."
      ],
      "metadata": {
        "id": "Ycrz994WKCjI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otZnqkxMD8Kv",
        "outputId": "d7c81706-62bf-4111-9ebf-ffe96d4d65fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of dataset:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Python Program: Logistic Regression for Multiclass Classification (One-vs-Rest)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset from sklearn (Iris dataset for multiclass classification)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"First 5 rows of dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Logistic Regression model with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8 - Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy."
      ],
      "metadata": {
        "id": "qgzzHro4KVZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Program: Hyperparameter Tuning for Logistic Regression using GridSearchCV\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset (Iris for multiclass classification)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/test split (though GridSearchCV will do CV internally)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],      # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],           # L1 = Lasso, L2 = Ridge\n",
        "    'solver': ['liblinear']            # Solver that supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "log_reg = LogisticRegression(max_iter=5000)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and best validation score\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W06XuvqTKPZH",
        "outputId": "45303bc5-995b-4c5c-e36d-31633c466c1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Cross-Validation Accuracy: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9 - Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling."
      ],
      "metadata": {
        "id": "3LAkrpa-Kupz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Program: Logistic Regression with and without Standardization\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Breast Cancer for binary classification)\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ----------- Logistic Regression WITHOUT Scaling -----------\n",
        "model_no_scaling = LogisticRegression(max_iter=5000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ----------- Logistic Regression WITH Scaling -----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(max_iter=5000)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", acc_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gYsmkruK051",
        "outputId": "7f2ab83c-4a83-4fdc-d27a-55711800fd4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "Accuracy with Scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10 - Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "Answer - If I were working at an e-commerce company and needed to predict which customers will respond to a marketing campaign using Logistic Regression, here’s how I’d approach it step by step:\n",
        "\n",
        "1. Understanding the Data\n",
        "\n",
        "The first thing I’d do is explore the dataset. Since only 5% of customers respond, the data is highly imbalanced. If I just train a plain Logistic Regression model, it will likely predict “no response” for almost everyone and still show high accuracy — but that won’t be useful for the business. What we care about is finding the right 5%.\n",
        "\n",
        "2. Feature Engineering & Scaling\n",
        "\n",
        "Next, I’d prepare the features.\n",
        "\n",
        "Customer data usually includes demographics (age, income, location), purchase history (frequency, average basket size, last purchase date), and engagement (email opens, clicks, app logins).\n",
        "\n",
        "Since Logistic Regression is sensitive to the scale of features, I’d standardize them using StandardScaler so that variables like “age” and “income” are on comparable scales.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "To deal with the 5% responders (minority class):\n",
        "\n",
        "I could use SMOTE (Synthetic Minority Oversampling Technique) to generate synthetic samples of responders.\n",
        "\n",
        "Alternatively, I’d try class weighting (setting class_weight='balanced' in Logistic Regression). This tells the model to “pay more attention” to the minority class without changing the data.\n",
        "\n",
        "Sometimes, a mix of undersampling the majority and oversampling the minority works best.\n",
        "\n",
        "4. Model Training with Logistic Regression\n",
        "\n",
        "I’d train a Logistic Regression model with L2 regularization (Ridge) to avoid overfitting.\n",
        "\n",
        "I’d tune the hyperparameter C (inverse of regularization strength) using GridSearchCV.\n",
        "\n",
        "I’d test both penalty='l1' and penalty='l2' to see which gives better performance.\n",
        "\n",
        "5. Evaluation Metrics\n",
        "\n",
        "Since accuracy is misleading for imbalanced data, I’d focus on:\n",
        "\n",
        "Precision (how many predicted responders were actually responders).\n",
        "\n",
        "Recall (how many actual responders we correctly identified).\n",
        "\n",
        "F1-score (balance between precision and recall).\n",
        "\n",
        "ROC-AUC score (how well the model separates responders vs non-responders).\n",
        "\n",
        "For the business case, recall might be more important because we don’t want to miss out on potential customers who could respond to the campaign.\n",
        "\n",
        "6. Business Perspective\n",
        "\n",
        "Finally, I’d explain the results to the marketing team in simple terms:\n",
        "\n",
        "Instead of sending emails to all customers, the model will generate a ranked list of likely responders.\n",
        "\n",
        "Even if we don’t perfectly predict all responders, identifying just 2x more responders than random guessing could save huge campaign costs and increase ROI."
      ],
      "metadata": {
        "id": "QIsVOx9WLf-M"
      }
    }
  ]
}